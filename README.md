# index-grabber

Рекурсивный скачиватель файлов с HTTP/HTTPS index-страниц (autoindex от nginx/apache).

## Возможности

- Рекурсивный обход директорий
- Докачка прерванных файлов (resume через Range)
- Параллельная загрузка в несколько потоков
- Автоматические повторы при ошибках сети/5xx/429
- Прогресс-бар с ETA и скоростью
- Фильтрация по regex (include/exclude)
- Пропуск уже скачанных файлов
- Защита от path traversal

## Установка

Зависимостей нет, только стандартная библиотека Python 3.6+.

```bash
git clone <repo-url>
cd download_ftp
```

## Использование

### Интерактивный режим

Просто запусти без аргументов:

```bash
python index-grabber.py
```

Скрипт спросит URL и папку для сохранения.

### CLI

```bash
python index-grabber.py "https://example.com/files/" -o ./downloads -t 8
```

### Параметры

| Параметр | По умолчанию | Описание |
|----------|--------------|----------|
| `url` | — | URL директории с листингом |
| `-o, --out` | `./downloads` | Папка для сохранения |
| `-t, --threads` | `4` | Количество потоков |
| `--timeout` | `30` | Таймаут соединения (сек) |
| `--retries` | `2` | Количество повторов при ошибке |
| `--retry-sleep` | `1.0` | Пауза между повторами (сек) |
| `--max-depth` | `-1` | Глубина рекурсии (-1 = без ограничений) |
| `--include` | — | Regex для фильтрации по имени файла |
| `--exclude` | — | Regex для исключения файлов |
| `--no-skip` | `false` | Не пропускать существующие файлы |
| `--ua` | `IndexGrabber/2.0` | User-Agent |
| `--log` | — | Путь к лог-файлу |
| `-v, --verbose` | `false` | Подробный вывод |

### Примеры

Скачать только `.zip` файлы:
```bash
python index-grabber.py "https://example.com/files/" --include "\.zip$"
```

Исключить временные файлы:
```bash
python index-grabber.py "https://example.com/files/" --exclude "\.(tmp|bak)$"
```

Ограничить глубину одним уровнем:
```bash
python index-grabber.py "https://example.com/files/" --max-depth 1
```

## Как это работает

1. Скрипт парсит HTML-страницу и собирает все `<a href>` ссылки
2. Рекурсивно обходит поддиректории (ссылки заканчивающиеся на `/`)
3. Для каждого файла проверяет размер через HEAD или Range-запрос
4. Скачивает файлы параллельно, сохраняя в `.part` до завершения
5. При обрыве — продолжает с места остановки

## Лицензия

MIT
